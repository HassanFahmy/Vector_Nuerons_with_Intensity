{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKHBaPjI/fyyiwaM+zDgWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HassanFahmy/Vector_Nuerons_with_Intensity/blob/main/3d_vector_nuerons_with_intensity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyNh1ZE21BkC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "EPS = 1e-6\n",
        "\n",
        "\n",
        "class VNLinear(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(VNLinear, self).__init__()\n",
        "        self.map_to_feat = nn.Linear(in_channels, out_channels, bias=False)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: point features of shape [B, N_feat, 5, N_samples, ...]\n",
        "        '''\n",
        "        x_out = self.map_to_feat(x.transpose(1,-1)).transpose(1,-1)\n",
        "        return x_out\n",
        "\n",
        "\n",
        "\n",
        "class VNLeakyReLU(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, share_nonlinearity=False, negative_slope=0.2):\n",
        "        super(VNLeakyReLU, self).__init__()\n",
        "        if share_nonlinearity == True:\n",
        "            self.map_to_dir = nn.Linear(in_channels, 1, bias=False)\n",
        "        else:\n",
        "            self.map_to_dir = nn.Linear(in_channels, in_channels, bias=False)\n",
        "        self.negative_slope = negative_slope\n",
        "        self.negative_slope = negative_slope\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: point features of shape [B, N_feat, 4, N_samples, ...]\n",
        "        '''\n",
        "        d = self.map_to_dir(x.transpose(1,-1)).transpose(1,-1)\n",
        "        dotprod = (x*d).sum(2, keepdim=True)\n",
        "        mask = (dotprod >= 0).float()\n",
        "        d_norm_sq = (d*d).sum(2, keepdim=True)\n",
        "        x_out = self.negative_slope * x + (1-self.negative_slope) * (mask*x + (1-mask)*(x-(dotprod/(d_norm_sq+EPS))*d))\n",
        "        x_out = self.negative_slope * x + (1-self.negative_slope) * (mask*x + (1-mask)*(x-(dotprod/(d_norm_sq+EPS))*d))\n",
        "        return x_out\n",
        "\n",
        "\n",
        "\n",
        "class VNLinearLeakyReLU(nn.Module): ########################################\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, dim=4, share_nonlinearity=False, negative_slope=0.2):\n",
        "        super(VNLinearLeakyReLU, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.negative_slope = negative_slope\n",
        "        self.negative_slope = negative_slope\n",
        "        \n",
        "        self.map_to_feat = nn.Linear(in_channels, out_channels, bias=False)\n",
        "        self.batchnorm = VNBatchNorm(out_channels, dim=dim)\n",
        "        \n",
        "        if share_nonlinearity == True:\n",
        "            self.map_to_dir = nn.Linear(in_channels, 1, bias=False)\n",
        "        else:\n",
        "            self.map_to_dir = nn.Linear(in_channels, out_channels, bias=False)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: point features of shape [B, N_feat, 3, N_samples, ...]\n",
        "        '''\n",
        "        # Linear\n",
        "        p = self.map_to_feat(x.transpose(1,-1)).transpose(1,-1)\n",
        "        \n",
        "        # BatchNorm\n",
        "        p = self.batchnorm(p)\n",
        "        # LeakyReLU\n",
        "        d = self.map_to_dir(x.transpose(1,-1)).transpose(1,-1)\n",
        "        dotprod = (p*d).sum(2, keepdims=True)\n",
        "        mask = (dotprod >= 0).float()\n",
        "        d_norm_sq = (d*d).sum(2, keepdims=True)\n",
        "        x_out = self.negative_slope * p + (1-self.negative_slope) * (mask*p + (1-mask)*(p-(dotprod/(d_norm_sq+EPS))*d))\n",
        "        x_out = self.negative_slope * p + (1-self.negative_slope) * (mask*p + (1-mask)*(p-(dotprod/(d_norm_sq+EPS))*d))\n",
        "        return x_out\n",
        "\n",
        "\n",
        "\n",
        "class VNLinearAndLeakyReLU(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, dim=5, share_nonlinearity=False, use_batchnorm='norm', negative_slope=0.2):\n",
        "        super(VNLinearLeakyReLU, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.share_nonlinearity = share_nonlinearity\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.negative_slope = negative_slope\n",
        "        self.negative_slope = negative_slope\n",
        "        \n",
        "        self.linear = VNLinear(in_channels, out_channels)\n",
        "        self.leaky_relu = VNLeakyReLU(out_channels, share_nonlinearity=share_nonlinearity, negative_slope=negative_slope)\n",
        "        self.leaky_relu = VNLeakyReLU(out_channels, share_nonlinearity=share_nonlinearity, negative_slope=negative_slope)\n",
        "        \n",
        "        # BatchNorm\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        if use_batchnorm != 'none':\n",
        "            self.batchnorm = VNBatchNorm(out_channels, dim=dim, mode=use_batchnorm)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: point features of shape [B, N_feat, 3, N_samples, ...]\n",
        "        '''\n",
        "        # Conv\n",
        "        x = self.linear(x)\n",
        "        # InstanceNorm\n",
        "        if self.use_batchnorm != 'none':\n",
        "            x = self.batchnorm(x)\n",
        "        # LeakyReLU\n",
        "        x_out = self.leaky_relu(x)\n",
        "        return x_out\n",
        "\n",
        "\n",
        "\n",
        "class VNBatchNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, num_features, dim):\n",
        "        super(VNBatchNorm, self).__init__()\n",
        "        self.dim = dim\n",
        "        if dim == 3 or dim == 4:\n",
        "            self.bn = nn.BatchNorm1d(num_features)\n",
        "        elif dim == 5:\n",
        "            self.bn = nn.BatchNorm2d(num_features)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: point features of shape [B, N_feat, 3, N_samples, ...]\n",
        "        '''\n",
        "        # norm = torch.sqrt((x*x).sum(2))\n",
        "        norm = torch.norm(x, dim=2) + EPS\n",
        "        norm_bn = self.bn(norm)\n",
        "        norm = norm.unsqueeze(2)\n",
        "        norm_bn = norm_bn.unsqueeze(2)\n",
        "        x = x / norm * norm_bn\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class VNMaxPool(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(VNMaxPool, self).__init__()\n",
        "        self.map_to_dir = nn.Linear(in_channels, in_channels, bias=False)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: point features of shape [B, N_feat, 3, N_samples, ...]\n",
        "        '''\n",
        "        d = self.map_to_dir(x.transpose(1,-1)).transpose(1,-1)\n",
        "        dotprod = (x*d).sum(2, keepdims=True)\n",
        "        idx = dotprod.max(dim=-1, keepdim=False)[1]\n",
        "        index_tuple = torch.meshgrid([torch.arange(j) for j in x.size()[:-1]]) + (idx,)\n",
        "        x_max = x[index_tuple]\n",
        "        return x_max\n",
        "\n",
        "\n",
        "def mean_pool(x, dim=-1, keepdim=False):\n",
        "    return x.mean(dim=dim, keepdim=keepdim)\n",
        "\n",
        "\n",
        "\n",
        "class VNStdFeature(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, dim=3, normalize_frame=False, share_nonlinearity=False, negative_slope=0.2):\n",
        "        super(VNStdFeature, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.normalize_frame = normalize_frame\n",
        "        \n",
        "        self.vn1 = VNLinearLeakyReLU(in_channels, in_channels//2, dim=dim, share_nonlinearity=share_nonlinearity, negative_slope=negative_slope)\n",
        "        self.vn2 = VNLinearLeakyReLU(in_channels//2, in_channels//4, dim=dim, share_nonlinearity=share_nonlinearity, negative_slope=negative_slope)\n",
        "        if normalize_frame:\n",
        "            self.vn_lin = nn.Linear(in_channels//4, 2, bias=False)\n",
        "        else:\n",
        "            self.vn_lin = nn.Linear(in_channels//4, 3, bias=False)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: point features of shape [B, N_feat, 3, N_samples, ...]\n",
        "        '''\n",
        "        z0 = x\n",
        "        z0 = self.vn1(z0)\n",
        "        z0 = self.vn2(z0)\n",
        "        z0 = self.vn_lin(z0.transpose(1, -1)).transpose(1, -1)\n",
        "\n",
        "        \n",
        "        if self.normalize_frame:\n",
        "            # make z0 orthogonal. u2 = v2 - proj_u1(v2)\n",
        "            v1 = z0[:,0,:]\n",
        "            #u1 = F.normalize(v1, dim=1)\n",
        "            v1_norm = torch.sqrt((v1*v1).sum(1, keepdims=True))\n",
        "            u1 = v1 / (v1_norm+EPS)\n",
        "            v2 = z0[:,1,:]\n",
        "            v2 = v2 - (v2*u1).sum(1, keepdims=True)*u1\n",
        "            #u2 = F.normalize(u2, dim=1)\n",
        "            v2_norm = torch.sqrt((v2*v2).sum(1, keepdims=True))\n",
        "            u2 = v2 / (v2_norm+EPS)\n",
        "\n",
        "            # compute the cross product of the two output vectors        \n",
        "            u3 = torch.cross(u1, u2)\n",
        "            z0 = torch.stack([u1, u2, u3], dim=1).transpose(1, 2)\n",
        "        else:\n",
        "            z0 = z0.transpose(1, 2)\n",
        "        if self.dim == 4:\n",
        "            x_std = torch.einsum('bijm,bjkm->bikm', x, z0)\n",
        "        elif self.dim == 3:\n",
        "            x_std = torch.einsum('bij,bjk->bik', x, z0)\n",
        "        elif self.dim == 5:\n",
        "            x_std = torch.einsum('bijmn,bjkmn->bikmn', x, z0)\n",
        "        \n",
        "        return x_std, z0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class get_model(nn.Module):\n",
        "\n",
        "    def __init__(self, num_class=10, normal_channel=False):\n",
        "        super(get_model, self).__init__()\n",
        "        self.n_knn = 5 #args.n_knn\n",
        "        \n",
        "        self.conv1 = VNLinearLeakyReLU(1, 64//4)\n",
        "        self.conv2 = VNLinearLeakyReLU(64//4, 64//4) \n",
        "        self.conv3 = VNLinearLeakyReLU(64//4, 128//4)\n",
        "        self.conv4 = VNLinearLeakyReLU(128//4, 256//4)\n",
        "        self.conv5 = VNLinearLeakyReLU(256//4+128//4+64//4+64//4, 1024//4, dim=4, share_nonlinearity=True)\n",
        "        \n",
        "        self.std_feature = VNStdFeature(1024//4*2, dim=3, normalize_frame=False)\n",
        "        self.linear1 = nn.Linear((1024//4)*4, 512)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dp1 = nn.Dropout(p=0.5)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dp2 = nn.Dropout(p=0.5)\n",
        "        self.linear3 = nn.Linear(256, num_class)\n",
        "        \n",
        "\n",
        "        self.pool1 = VNMaxPool(64//4)\n",
        "        self.pool2 = VNMaxPool(64//4)\n",
        "        self.pool3 = VNMaxPool(128//4)\n",
        "        self.pool4 = VNMaxPool(256//4)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv1(x)\n",
        "        x1 = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x2 = self.pool2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x3 = self.pool3(x)\n",
        "        \n",
        "        x = self.conv4(x)\n",
        "        x4 = self.pool4(x)\n",
        "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
        "        x = self.conv5(x)\n",
        "        \n",
        "        num_points = x.size(-1)\n",
        "        x_mean = x.mean(dim=-1, keepdim=True).expand(x.size())\n",
        "        x = torch.cat((x, x_mean), 1)\n",
        "\n",
        "        x, trans = self.std_feature(x)\n",
        "        #swap x with trans?\n",
        "        #x = x.view(batch_size, -1, num_points)\n",
        "        \n",
        "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
        "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n",
        "        x = torch.cat((x1, x2), 1)\n",
        "        x = F.leaky_relu(self.bn1(self.linear1(x)), negative_slope=0.2)\n",
        "        x = self.dp1(x)\n",
        "        x = F.leaky_relu(self.bn2(self.linear2(x)), negative_slope=0.2)\n",
        "        x = self.dp2(x)\n",
        "        x = self.linear3(x)\n",
        "        \n",
        "        trans_feat = None\n",
        "        return x, trans_feat\n",
        "\n",
        "\n",
        "\n",
        "class get_loss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(get_loss, self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, pred, target, trans_feat, smoothing=True):\n",
        "        ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
        "\n",
        "        target = target.contiguous().view(-1)\n",
        "\n",
        "        if smoothing:\n",
        "            eps = 0.2\n",
        "            n_class = pred.size(1)\n",
        "\n",
        "            one_hot = torch.zeros_like(pred).scatter(1, target.view(-1, 1), 1)\n",
        "            one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
        "            log_prb = F.log_softmax(pred, dim=1)\n",
        "\n",
        "            loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
        "        else:\n",
        "            loss = F.cross_entropy(pred, target, reduction='mean')\n",
        "            \n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "upSDx1uPteQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class get_model(nn.Module):\n",
        "\n",
        "    def __init__(self, num_class=10, normal_channel=False):\n",
        "        super(get_model, self).__init__()\n",
        "        self.n_knn = 5 #args.n_knn\n",
        "        \n",
        "        self.conv1 = VNLinearLeakyReLU(1, 16)\n",
        "        self.conv2 = VNLinearLeakyReLU(16, 32) \n",
        "        self.conv3 = VNLinearLeakyReLU(32, 64)\n",
        "        self.conv4 = VNLinearLeakyReLU(64, 16)\n",
        "        self.conv5 = VNLinearLeakyReLU(16, 64)\n",
        "        self.conv6 = VNLinearLeakyReLU(64, 32)\n",
        "        self.conv7 = VNLinearLeakyReLU(32, 16)\n",
        "\n",
        "        self.std_feature = VNStdFeature(16*2, dim=3, normalize_frame=False)\n",
        "        self.linear1 = nn.Linear((1024//4)*4, 512)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dp1 = nn.Dropout(p=0.5)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dp2 = nn.Dropout(p=0.5)\n",
        "        self.linear3 = nn.Linear(256, num_class)\n",
        "        \n",
        "\n",
        "        self.pool1 = VNMaxPool(64//4)\n",
        "        self.pool2 = VNMaxPool(64//4)\n",
        "        self.pool3 = VNMaxPool(128//4)\n",
        "        self.pool4 = VNMaxPool(256//4)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv1(x)\n",
        "        x1 = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x2 = self.pool2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x3 = self.pool3(x)\n",
        "        \n",
        "        x = self.conv4(x)\n",
        "        x4 = self.pool4(x)\n",
        "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
        "        x = self.conv5(x)\n",
        "        \n",
        "        num_points = x.size(-1)\n",
        "        x_mean = x.mean(dim=-1, keepdim=True).expand(x.size())\n",
        "        x = torch.cat((x, x_mean), 1)\n",
        "\n",
        "        x, trans = self.std_feature(x)\n",
        "        #swap x with trans?\n",
        "        #x = x.view(batch_size, -1, num_points)\n",
        "        \n",
        "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
        "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n",
        "        x = torch.cat((x1, x2), 1)\n",
        "        x = F.leaky_relu(self.bn1(self.linear1(x)), negative_slope=0.2)\n",
        "        x = self.dp1(x)\n",
        "        x = F.leaky_relu(self.bn2(self.linear2(x)), negative_slope=0.2)\n",
        "        x = self.dp2(x)\n",
        "        x = self.linear3(x)\n",
        "        \n",
        "        trans_feat = None\n",
        "        return x, trans_feat\n",
        "\n",
        "\n",
        "\n",
        "class get_loss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(get_loss, self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, pred, target, trans_feat, smoothing=True):\n",
        "        ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
        "\n",
        "        target = target.contiguous().view(-1)\n",
        "\n",
        "        if smoothing:\n",
        "            eps = 0.2\n",
        "            n_class = pred.size(1)\n",
        "\n",
        "            one_hot = torch.zeros_like(pred).scatter(1, target.view(-1, 1), 1)\n",
        "            one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
        "            log_prb = F.log_softmax(pred, dim=1)\n",
        "\n",
        "            loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
        "        else:\n",
        "            loss = F.cross_entropy(pred, target, reduction='mean')\n",
        "            \n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "h7c05eY8LG-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = get_model()\n",
        "criterion = get_loss()\n",
        "optimizer = torch.optim.SGD(\n",
        "            classifier.parameters(),\n",
        "            lr=0.01,\n",
        "            momentum=0.9,\n",
        "            weight_decay=1e-4\n",
        "        )\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)\n",
        "global_epoch = 0\n",
        "global_step = 0\n",
        "best_instance_acc = 0.0\n",
        "best_class_acc = 0.0\n",
        "mean_correct = []"
      ],
      "metadata": {
        "id": "_nNG-fS95Lcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5,40,4)\n",
        "emb ,feat = classifier(x)\n",
        "print (emb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "CffjPUPuM5Bc",
        "outputId": "b4ad9525-6860-44cc-d9b0-e45d4a8c641d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-ca2add3552dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0memb\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-63ed9ffec321>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-2b0d1f6d8de5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         '''\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_to_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mdotprod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdotprod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (800x32 and 16x16)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ToPC(object):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        ret = torch.tensor([[[i/sample.shape[1],j/sample.shape[2],sample[b][i][j]]for i in range(sample.shape[1])for j in range(sample.shape[2]) ]for b in range(sample.shape[0])])\n",
        "        return ret"
      ],
      "metadata": {
        "id": "LKImiCbw2kfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.RandomRotation((-90,90)),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,)),\n",
        "                               ToPC(),\n",
        "                             ])),\n",
        "  batch_size=20, shuffle=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=1, shuffle=True)\n",
        "\n",
        "\"\"\"\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "print (\"images\", images.shape, \"labels:\", labels)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "print(\"images shape\", images.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfoYdwBEzf9E",
        "outputId": "c42fd417-8447-45a0-90ea-aae1bb4173da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images shape torch.Size([20, 1, 784, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch3d"
      ],
      "metadata": {
        "id": "8zO6TIWSXNbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch3d.transforms import RotateAxisAngle, Rotate, random_rotations\n",
        "x = torch.rand(10,40,3)\n",
        "emb ,feat = classifier(x)\n",
        "trot = RotateAxisAngle(angle=90, axis=\"Z\", degrees=True)\n",
        "x2 = trot.transform_points(x)\n",
        "emb2 ,feat = classifier(x2)\n",
        "inemb1 = trot.transform_points(emb)\n",
        "print (emb2-inemb1)\n",
        "print(emb2)"
      ],
      "metadata": {
        "id": "r-27uflUXkuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch3d.transforms import RotateAxisAngle, Rotate, random_rotations\n",
        "\n",
        "rot = 'so3'\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "        print('Epoch %d (%d/%s):' % (global_epoch + 1, epoch + 1, args.epoch))\n",
        "\n",
        "        scheduler.step()\n",
        "        for batch_id, data in tqdm(enumerate(trainDataLoader, 0), total=len(trainDataLoader), smoothing=0.9):\n",
        "            points, target = data\n",
        "            \n",
        "            trot = None\n",
        "            if rot == 'z':\n",
        "                trot = RotateAxisAngle(angle=torch.rand(points.shape[0])*360, axis=\"Z\", degrees=True)\n",
        "                trot = RotateAxisAngle(angle=torch.rand(points.shape[0])*360, axis=\"Z\", degrees=True)\n",
        "            elif rot == 'so3':\n",
        "                trot = Rotate(R=random_rotations(points.shape[0]))\n",
        "                trot = Rotate(R=random_rotations(points.shape[0]))\n",
        "            if trot is not None:\n",
        "                points = trot.transform_points(points)\n",
        "            \n",
        "            points = points.data.numpy()\n",
        "            points = provider.random_point_dropout(points)\n",
        "            points[:,:, 0:3] = provider.random_scale_point_cloud(points[:,:, 0:3])\n",
        "            points[:,:, 0:3] = provider.shift_point_cloud(points[:,:, 0:3])\n",
        "            points = torch.Tensor(points)\n",
        "            target = target[:, 0]\n",
        "\n",
        "            points = points.transpose(2, 1)\n",
        "            points, target = points.cuda(), target.cuda()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            classifier = classifier.train()\n",
        "            pred, trans_feat = classifier(points)\n",
        "            loss = criterion(pred, target.long(), trans_feat)\n",
        "            pred_choice = pred.data.max(1)[1]\n",
        "            correct = pred_choice.eq(target.long().data).cpu().sum()\n",
        "            mean_correct.append(correct.item() / float(points.size()[0]))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            global_step += 1\n",
        "\n",
        "        train_instance_acc = np.mean(mean_correct)\n",
        "        print('Train Instance Accuracy: %f' % train_instance_acc)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            instance_acc, class_acc = test(classifier.eval(), testDataLoader)\n",
        "\n",
        "            if (instance_acc >= best_instance_acc):\n",
        "                best_instance_acc = instance_acc\n",
        "                best_epoch = epoch + 1\n",
        "\n",
        "            if (class_acc >= best_class_acc):\n",
        "                best_class_acc = class_acc\n",
        "            log_string('Test Instance Accuracy: %f, Class Accuracy: %f'% (instance_acc, class_acc))\n",
        "            log_string('Best Instance Accuracy: %f, Class Accuracy: %f'% (best_instance_acc, best_class_acc))\n",
        "\n",
        "            if (instance_acc >= best_instance_acc):\n",
        "                logger.info('Save model...')\n",
        "                savepath = str(checkpoints_dir) + '/best_model.pth'\n",
        "                log_string('Saving at %s'% savepath)\n",
        "                state = {\n",
        "                    'epoch': best_epoch,\n",
        "                    'instance_acc': instance_acc,\n",
        "                    'class_acc': class_acc,\n",
        "                    'model_state_dict': classifier.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                }\n",
        "                torch.save(state, savepath)\n",
        "            global_epoch += 1\n"
      ],
      "metadata": {
        "id": "KzhpP2ybPJ1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph_feature(x, k=5, idx=None, x_coord=None):\n",
        "    batch_size = x.size(0)\n",
        "    num_points = x.size(2)\n",
        "    x = x.view(batch_size, -1, num_points)\n",
        "    if idx is None:\n",
        "        if x_coord is None: # dynamic knn graph\n",
        "            idx = knn(x, k=k)\n",
        "        else:          # fixed knn graph with input point coordinates\n",
        "            idx = knn(x_coord, k=k)\n",
        "\n",
        "    idx_base = torch.arange(0, batch_size).view(-1, 1, 1)*num_points\n",
        "\n",
        "    idx = idx + idx_base\n",
        "\n",
        "    idx = idx.view(-1)\n",
        " \n",
        "    _, num_dims, _ = x.size()\n",
        "\n",
        "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
        "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
        "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
        "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
        "    \n",
        "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
        "  \n",
        "    return feature\n",
        "\n",
        "def knn(x, k):\n",
        "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
        "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
        "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
        " \n",
        "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "NKJuGPBX4Uev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}